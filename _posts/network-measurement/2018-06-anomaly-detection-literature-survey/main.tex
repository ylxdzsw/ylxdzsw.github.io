\documentclass{manuscript}

\usepackage{textcomp}
\usepackage{enumitem}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{indentfirst}

\title{Literature Survey of Network Anomaly Detection}
\author{Zhang Shiwei}
\date{June 2018}

\begin{document}
    \maketitle

    \section{Paper Review}

    \subsection{Network Fault Diagnosis Using Data Mining Classifiers \cite{rozaki_network_2015}}

    This paper was presented in AIRCC, 2015 by Eleni Rozaki from the Cardiff University.

    The first section describes the FCAPS framework and the position of their contribution under that framework. The
    FCAPS framework stands for fault, configuration, accounting, performance, and security. Their work focus on fault
    diagnosis.

    The second section is the general process of data mining, i.e., data cleaning, section, pattern mining, and knowledge
    representation. They use Weka to perform the mining.

    In the next section several data mining techniques were explained and compaired:
    \begin{description}[noitemsep, nolistsep]
        \item[J48 tree] (more commonly known as C4.5). It builds decision trees by maximizing information gain greedly at
                        each node.\cite{quinlan_c4.5:_1993}
        \item[LAD tree] Inducing ADTrees using LogitBoost. An ADTree consists of an alternation of decision nodes, which
                        specify a predicate condition, and prediction nodes, which contain a single number. An instance
                        is classified by an ADTree by following all paths for which all decision nodes are true, and
                        summing any prediction nodes that are traversed.\cite{holmes_multiclass_2002}
        \item[JRip] Alternatively grow and prune rules to build an initial rule set in terms of information gain, Then
                    examine each rule by generate two variants of each rule from randomized data, see which have shorter
                    descrition length.\cite{cohen_fast_1995}
        \item[PART] Generating a decision list by buiding a C4.5 decision tree in each iteration and makes the "best" leaf
                    into a rule. Instances are classified at the first match.\cite{frank_generating_1998}
        \item[Na√Øve Bayes] Using Bayes rule to calculate the conditional probability with the assumption that all
                           attributes are independent of each other.\cite{john_estimating_1995}
        \item[Bayesnet] Also known as belief networks. It use Bayes rule recursively in a DAG to infer the probabilities
                        of the state of a node.\cite{barco_comparison_2006}
    \end{description}

    In the fourth section some definitions are given. The most important concept is KPI, which acts as the target value
    to predict. They define KPI as a variable takes 3 possible values: Normal, Critical and Warning. The value of KPI is
    determined by DCR (Call Drop Rate), CSSR (Call set up success rate), TR (Traffic Rate), and HOF (Handover Faulures)
    empirically.

    In the fifth and sixth sections the authors showed their results by screenshots of Weka outputs, and made several
    comparisons between above algorithms.

    \subsection{Detecting and Localizing End-to-End Performance Degradation for Cellular Data Services \cite{ahmed_detecting_2016}}

    This paper was presented in INFOCOM, 2016 by Michigan State University and AT\&T.

    Firstly they stated the goal, which is mainly to ascribe E2E performance degradations to one of the four factors:
    application type, content provider, mobile device, and user location.

    Next they gave an overview of their method. The first step is to build 24 * 7 models that predicting the performance
    of the instances correspond to a specific hour in a week. Then, use these predictions to define degardation. Finally
    use association rules mining to find patterns that cause degradations.

    In the rest of Section 1 they described the 3 main challenges and their solutions. The first challenge is data sparsity.
    They use recursive grouping to handle this. The second challenge is to localize the cause of degardation, which is
    what they deploy the association for. The last one is to quantitatively evaluate the result, they solve this by mannually
    inspecting some cases and injecting synthetic cases which act as ground truth.

    In the second section the authors discussed some related works in network diagnosis and performance measurement. Their
    work is unique in that they use association rules to find the root cause.

    In the third section they introduced the collection and basic analysis of data. The data were collected from between
    SGW and PGW within a core GPRS network of a US cellular service provider. The data contain TCP level information. The
    E2E performance metrics consist of TCP loss ratio and RTT. The TCP loss ratio is defined as $\frac{\text{bytes retransmitted}}
    {\text{actual bytes in the flow}}$, where retransmissions are detected by tracking packet sequence numbers. The RTT
    is splited to cellular network side RTT and internet side RTT.

    The fourth section is the major part where the whole process was described in detail.

    \textbf{performance matrix:} They first calculated $E_A = [1..24 * 7, \{\mathbb{L}, \mathbb{C}, \mathbb{D}, \mathbb{A}\}]$,
    where $E_A[i, \mathbb{X}]$ is a vector of a length $X$ vector contains of the median values in W weeks.

    \textbf{remove outliers:} Outliers are identified by robust regression, which use iteratively re-weighted least squares
    (IRLS) to find a weight that minimizes the impact of extreme data points.

    \textbf{E2E matrix:} Next they defined the E2E matrix as $E_I = [1..L, 1..P, 1..D, 1..A]$. The element type depends.

    \textbf{deviating E2E instance identification:} First they defined $\bar{E_A}$ which differs from $E_A$ only in that
    it contains the standard deviation of the W values whereas $E_A$ contains the medieans. Then, they choose
    $[\text{predicted performance} - 2\sigma, \text{predicted performance} + 2\sigma]$ as the definition of
    ``deviating too much''. For each E2E instance, if it have deviating performance in more than 50\% in the $24*7$ hours,
    it will be labeled as deviating.

    \textbf{grouping} Association rule mining techniques were deployed to perform grouping. The transactions is a list of
    $\langle l, p, d, a, c \rangle$, where $c$ is where it is deviating or not. Then, the classic Apriori algorithm is
    used to find the rules. Each rule like $\text{NewYork}, \text{Google}, \text{iPhone} \rightarrow \text{deviate}$
    corresponds to a group $\text{NewYork, Google, iPhone,} \ast$.

    \textbf{further selection} They used a complex method to further reduce the number of groups and determine the model
    to use for instances that belongs to more than one group.

    \textbf{performance degradation detection} The performance degradation is detected by compairing the performance of
    latest hour with $\text{predicted performance} + 2\sigma$ of the same hour. The association rule mining techniques
    then used again to find the cause of degradation in the latest hour.

    Finally they made some evaluations. First they inspected the accuracy using synthetic data, then tried the model in
    the wild and made some explainations to the result.

    \subsection{Highlights of Other Related Works}

    \cite{hutchison_mobile_2014} They collect data from end devices too. Their data is more continuous and have more
    performance-related information.

    \section{Relative Work Summary}

    \section{My Proposal}

    \subsection{FFM\cite{juan_field-aware_2016}}

    Field-aware factorization machine (FFM) is a model that is good at handling sparse catogorical features.
    $$ y(x) = w_0 + \sum^n_{i=1}w_ix_i + \sum^n_{i=1}\sum^n_{j=i+1}\langle \mathbf{v}_{i, f_j}, \mathbf{v}_{j, f_i}\rangle x_ix_j $$
    $f_i$ is the field of the i-th feature. By sorting $w_i$ and $\langle \mathbf{v}_{i, f_j}, \mathbf{v}_{j, f_i}\rangle$,
    we can find out what combinations of features will cause RTT to be high.

    \bibliographystyle{unsrt}
    \bibliography{main}
\end{document}
