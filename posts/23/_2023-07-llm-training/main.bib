@article{bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@article{gpt3,
  title={Language models are few-shot learners},
  author={Brown, Tom B and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={arXiv preprint arXiv:2005.14165},
  year={2020}
}

@article{flexflow,
  title={Beyond data and model parallelism for deep neural networks},
  author={Jia, Zhihao and Zaharia, Matei and Aiken, Alex},
  journal={arXiv preprint arXiv:1807.05358},
  year={2018}
}

@article{m6,
  title={M6: A chinese multimodal pretrainer},
  author={Lin, Junyang and Men, Rui and Yang, An and Zhou, Chang and Ding, Ming and Zhang, Yichang and Wang, Peng and Wang, Ang and Jiang, Le and Jia, Xianyan and others},
  journal={arXiv preprint arXiv:2103.00823},
  year={2021}
}

@article{gshard,
  title={Gshard: Scaling giant models with conditional computation and automatic sharding},
  author={Lepikhin, Dmitry and Lee, HyoukJoong and Xu, Yuanzhong and Chen, Dehao and Firat, Orhan and Huang, Yanping and Krikun, Maxim and Shazeer, Noam and Chen, Zhifeng},
  journal={arXiv preprint arXiv:2006.16668},
  year={2020}
}

@article{gspmd,
  title={GSPMD: General and Scalable Parallelization for ML Computation Graphs},
  author={Xu, Yuanzhong and Lee, HyoukJoong and Chen, Dehao and Hechtman, Blake and Huang, Yanping and Joshi, Rahul and Krikun, Maxim and Lepikhin, Dmitry and Ly, Andy and Maggioni, Marcello and others},
  journal={arXiv preprint arXiv:2105.04663},
  year={2021}
}

@inproceedings{zero,
  title={Zero: Memory optimizations toward training trillion parameter models},
  author={Rajbhandari, Samyam and Rasley, Jeff and Ruwase, Olatunji and He, Yuxiong},
  booktitle={SC20: International Conference for High Performance Computing, Networking, Storage and Analysis},
  pages={1--16},
  year={2020},
  organization={IEEE}
}

@article{zero-offload,
  title={Zero-offload: Democratizing billion-scale model training},
  author={Ren, Jie and Rajbhandari, Samyam and Aminabadi, Reza Yazdani and Ruwase, Olatunji and Yang, Shuangyan and Zhang, Minjia and Li, Dong and He, Yuxiong},
  journal={arXiv preprint arXiv:2101.06840},
  year={2021}
}

@inproceedings{deepspeed,
  title={Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters},
  author={Rasley, Jeff and Rajbhandari, Samyam and Ruwase, Olatunji and He, Yuxiong},
  booktitle={Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
  pages={3505--3506},
  year={2020}
}

@article{megatron,
  title={Megatron-lm: Training multi-billion parameter language models using model parallelism},
  author={Shoeybi, Mohammad and Patwary, Mostofa and Puri, Raul and LeGresley, Patrick and Casper, Jared and Catanzaro, Bryan},
  journal={arXiv preprint arXiv:1909.08053},
  year={2019}
}

@article{mtf,
  title={Mesh-tensorflow: Deep learning for supercomputers},
  author={Shazeer, Noam and Cheng, Youlong and Parmar, Niki and Tran, Dustin and Vaswani, Ashish and Koanantakool, Penporn and Hawkins, Peter and Lee, HyoukJoong and Hong, Mingsheng and Young, Cliff and others},
  journal={arXiv preprint arXiv:1811.02084},
  year={2018}
}

@article{gpipe,
  title={Gpipe: Efficient training of giant neural networks using pipeline parallelism},
  author={Huang, Yanping and Cheng, Youlong and Bapna, Ankur and Firat, Orhan and Chen, Dehao and Chen, Mia and Lee, HyoukJoong and Ngiam, Jiquan and Le, Quoc V and Wu, Yonghui and others},
  journal={Advances in neural information processing systems},
  volume={32},
  pages={103--112},
  year={2019}
}

@inproceedings{pipedream,
  title={PipeDream: generalized pipeline parallelism for DNN training},
  author={Narayanan, Deepak and Harlap, Aaron and Phanishayee, Amar and Seshadri, Vivek and Devanur, Nikhil R and Ganger, Gregory R and Gibbons, Phillip B and Zaharia, Matei},
  booktitle={Proceedings of the 27th ACM Symposium on Operating Systems Principles},
  pages={1--15},
  year={2019}
}

@inproceedings{pace,
  title={Preemptive all-reduce scheduling for expediting distributed dnn training},
  author={Bao, Yixin and Peng, Yanghua and Chen, Yangrui and Wu, Chuan},
  booktitle={IEEE INFOCOM 2020-IEEE Conference on Computer Communications},
  pages={626--635},
  year={2020},
  organization={IEEE}
}

@article{pangu,
  title={PanGu-$\alpha$: Large-scale Autoregressive Pretrained Chinese Language Models with Auto-parallel Computation},
  author={Zeng, Wei and Ren, Xiaozhe and Su, Teng and Wang, Hui and Liao, Yi and Wang, Zhiwei and Jiang, Xin and Yang, ZhenZhang and Wang, Kaisheng and Zhang, Xiaoda and others},
  journal={arXiv preprint arXiv:2104.12369},
  year={2021}
}

@article{optimus,
  title={An Efficient 2D Method for Training Super-Large Deep Learning Models},
  author={Xu, Qifan and Li, Shenggui and Gong, Chaoyu and You, Yang},
  journal={arXiv preprint arXiv:2104.05343},
  year={2021}
}

@article{horovod,
  Author = {Alexander Sergeev and Mike Del Balso},
  Journal = {arXiv preprint arXiv:1802.05799},
  Title = {Horovod: fast and easy distributed deep learning in {TensorFlow}},
  Year = {2018}
}

@inproceedings{bytescheduler,
  title={A generic communication scheduler for distributed dnn training acceleration},
  author={Peng, Yanghua and Zhu, Yibo and Chen, Yangrui and Bao, Yixin and Yi, Bairen and Lan, Chang and Wu, Chuan and Guo, Chuanxiong},
  booktitle={Proceedings of the 27th ACM Symposium on Operating Systems Principles},
  pages={16--29},
  year={2019}
}

@inproceedings{byteps,
  title={A unified architecture for accelerating distributed DNN training in heterogeneous GPU/CPU clusters},
  author={Jiang, Yimin and Zhu, Yibo and Lan, Chang and Yi, Bairen and Cui, Yong and Guo, Chuanxiong},
  booktitle={Proceedings of the 14th USENIX Conference on Operating Systems Design and Implementation},
  pages={463--479},
  year={2020}
}

@article{switchtransformer,
  title={Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity},
  author={Fedus, William and Zoph, Barret and Shazeer, Noam},
  journal={arXiv preprint arXiv:2101.03961},
  year={2021}
}

@article{moe,
  title={Outrageously large neural networks: The sparsely-gated mixture-of-experts layer},
  author={Shazeer, Noam and Mirhoseini, Azalia and Maziarz, Krzysztof and Davis, Andy and Le, Quoc and Hinton, Geoffrey and Dean, Jeff},
  journal={arXiv preprint arXiv:1701.06538},
  year={2017}
}

@article{gradientcheckpointing,
  title={Training deep nets with sublinear memory cost},
  author={Chen, Tianqi and Xu, Bing and Zhang, Chiyuan and Guestrin, Carlos},
  journal={arXiv preprint arXiv:1604.06174},
  year={2016}
}

@inproceedings{inceptionv3,
  title={Rethinking the inception architecture for computer vision},
  author={Szegedy, Christian and Vanhoucke, Vincent and Ioffe, Sergey and Shlens, Jon and Wojna, Zbigniew},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={2818--2826},
  year={2016}
}

@inproceedings{tofu,
  title={Supporting very large models using automatic dataflow graph partitioning},
  author={Wang, Minjie and Huang, Chien-chin and Li, Jinyang},
  booktitle={Proceedings of the Fourteenth EuroSys Conference 2019},
  pages={1--17},
  year={2019}
}

@article{zcode,
  title={Scalable and efficient moe training for multitask multilingual models},
  author={Kim, Young Jin and Awan, Ammar Ahmad and Muzio, Alexandre and Salinas, Andres Felipe Cruz and Lu, Liyang and Hendy, Amr and Rajbhandari, Samyam and He, Yuxiong and Awadalla, Hany Hassan},
  journal={arXiv preprint arXiv:2109.10465},
  year={2021}
}

@article{fastmoe,
  title={FastMoE: A Fast Mixture-of-Expert Training System},
  author={He, Jiaao and Qiu, Jiezhong and Zeng, Aohan and Yang, Zhilin and Zhai, Jidong and Tang, Jie},
  journal={arXiv preprint arXiv:2103.13262},
  year={2021}
}

@article{pytorch,
  title={Pytorch: An imperative style, high-performance deep learning library},
  author={Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and others},
  journal={Advances in neural information processing systems},
  volume={32},
  pages={8026--8037},
  year={2019}
}

@inproceedings{tensorflow,
  title={Tensorflow: A system for large-scale machine learning},
  author={Abadi, Mart{\'\i}n and Barham, Paul and Chen, Jianmin and Chen, Zhifeng and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Irving, Geoffrey and Isard, Michael and others},
  booktitle={12th $\{$USENIX$\}$ symposium on operating systems design and implementation ($\{$OSDI$\}$ 16)},
  year={2016}
}

@inproceedings{hypar,
  title={Hypar: Towards hybrid parallelism for deep learning accelerator array},
  author={Song, Linghao and Mao, Jiachen and Zhuo, Youwei and Qian, Xuehai and Li, Hai and Chen, Yiran},
  booktitle={2019 IEEE International Symposium on High Performance Computer Architecture (HPCA)},
  pages={56--68},
  year={2019},
  organization={IEEE}
}

@inproceedings{accpar,
  title={Accpar: Tensor partitioning for heterogeneous deep learning accelerators},
  author={Song, Linghao and Chen, Fan and Zhuo, Youwei and Qian, Xuehai and Li, Hai and Chen, Yiran},
  booktitle={2020 IEEE International Symposium on High Performance Computer Architecture (HPCA)},
  pages={342--355},
  year={2020},
  organization={IEEE}
}

@inproceedings{prague,
  title={Prague: High-performance heterogeneity-aware asynchronous decentralized training},
  author={Luo, Qinyi and He, Jiaao and Zhuo, Youwei and Qian, Xuehai},
  booktitle={Proceedings of the Twenty-Fifth International Conference on Architectural Support for Programming Languages and Operating Systems},
  pages={401--416},
  year={2020}
}

@article{colossalai,
  title={Colossal-AI: A Unified Deep Learning System For Large-Scale Parallel Training},
  author={Bian, Zhengda and Liu, Hongxin and Wang, Boxiang and Huang, Haichen and Li, Yongbin and Wang, Chuanrui and Cui, Fan and You, Yang},
  journal={arXiv preprint arXiv:2110.14883},
  year={2021}
}

@inproceedings{heterog,
  title={Optimizing distributed training deployment in heterogeneous GPU clusters},
  author={Yi, Xiaodong and Zhang, Shiwei and Luo, Ziyue and Long, Guoping and Diao, Lansong and Wu, Chuan and Zheng, Zhen and Yang, Jun and Lin, Wei},
  booktitle={Proceedings of the 16th International Conference on emerging Networking EXperiments and Technologies},
  pages={93--107},
  year={2020}
}

@article{paretodp,
  title={Pareto optimization in algebraic dynamic programming},
  author={Saule, C{\'e}dric and Giegerich, Robert},
  journal={Algorithms for Molecular Biology},
  volume={10},
  number={1},
  pages={1--20},
  year={2015},
  publisher={BioMed Central}
}

@inproceedings{p3,
  title={P3: Distributed deep graph learning at scale},
  author={Gandhi, Swapnil and Iyer, Anand Padmanabha},
  booktitle={15th $\{$USENIX$\}$ Symposium on Operating Systems Design and Implementation ($\{$OSDI$\}$ 21)},
  pages={551--568},
  year={2021}
}

@inproceedings{staleness,
  title={Model accuracy and runtime tradeoff in distributed deep learning: A systematic study},
  author={Gupta, Suyog and Zhang, Wei and Wang, Fei},
  booktitle={2016 IEEE 16th International Conference on Data Mining (ICDM)},
  pages={171--180},
  year={2016},
  organization={IEEE}
}

@article{adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}

@article{adagrad,
  title={Adaptive subgradient methods for online learning and stochastic optimization.},
  author={Duchi, John and Hazan, Elad and Singer, Yoram},
  journal={Journal of machine learning research},
  volume={12},
  number={7},
  year={2011}
}

@misc{mpi,
  title={MPI: A message-passing interface standard},
  author={MPI{\ }Forum},
  year={1994}
}

@article{alpa,
  title={Alpa: Automating Inter-and Intra-Operator Parallelism for Distributed Deep Learning},
  author={Zheng, Lianmin and Li, Zhuohan and Zhang, Hao and Zhuang, Yonghao and Chen, Zhifeng and Huang, Yanping and Wang, Yida and Xu, Yuanzhong and Zhuo, Danyang and Gonzalez, Joseph E and others},
  journal={arXiv preprint arXiv:2201.12023},
  year={2022}
}

@article{wikitext,
  title={Pointer sentinel mixture models},
  author={Merity, Stephen and Xiong, Caiming and Bradbury, James and Socher, Richard},
  journal={arXiv preprint arXiv:1609.07843},
  year={2016}
}

@article{vit,
  title={An image is worth 16x16 words: Transformers for image recognition at scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
  journal={arXiv preprint arXiv:2010.11929},
  year={2020}
}

@article{cifar,
  title={Learning multiple layers of features from tiny images},
  author={Krizhevsky, Alex and Hinton, Geoffrey and others},
  year={2009},
  publisher={Citeseer}
}

@article{gdp,
  title={Gdp: Generalized device placement for dataflow graphs},
  author={Zhou, Yanqi and Roy, Sudip and Abdolrashidi, Amirali and Wong, Daniel and Ma, Peter C and Xu, Qiumin and Zhong, Ming and Liu, Hanxiao and Goldie, Anna and Mirhoseini, Azalia and others},
  journal={arXiv preprint arXiv:1910.01578},
  year={2019}
}

@article{placeto,
  title={Placeto: Learning generalizable device placement algorithms for distributed machine learning},
  author={Addanki, Ravichandra and Venkatakrishnan, Shaileshh Bojja and Gupta, Shreyan and Mao, Hongzi and Alizadeh, Mohammad},
  journal={arXiv preprint arXiv:1906.08879},
  year={2019}
}

@article{iperf3,
  title={iperf3, tool for active measurements of the maximum achievable bandwidth on ip networks},
  author={Dugan, Jon and Elliott, Seth and Mah, Bruce A and Poskanzer, Jeff and Prabhu, Kaustubh},
  journal={URL: https://github.com/esnet/iperf},
}

@article{iproute2,
  title={Linux routing utilities},
  author={Kuznetsov, Alexey and Hemminger, Stephen},
  journal={URL https://github.com/shemminger/iproute2}
}

@article{ga,
  title={Gradient accumulation PyTorch},
  author={PyTorch},
  journal={URL https://gist.github.com/thomwolf/ac7a7da6b1888c2eeac8ac8b9b05d3d3}
}

@inproceedings{hoplite,
  title={Hoplite: efficient and fault-tolerant collective communication for task-based distributed systems},
  author={Zhuang, Siyuan and Li, Zhuohan and Zhuo, Danyang and Wang, Stephanie and Liang, Eric and Nishihara, Robert and Moritz, Philipp and Stoica, Ion},
  booktitle={Proceedings of the 2021 ACM SIGCOMM 2021 Conference},
  pages={641--656},
  year={2021}
}

@inproceedings{dgcl,
  title={DGCL: An efficient communication library for distributed GNN training},
  author={Cai, Zhenkun and Yan, Xiao and Wu, Yidi and Ma, Kaihao and Cheng, James and Yu, Fan},
  booktitle={Proceedings of the Sixteenth European Conference on Computer Systems},
  pages={130--144},
  year={2021}
}

@article{virtualflow,
  title={VirtualFlow: Decoupling Deep Learning Models from the Underlying Hardware},
  author={Or, Andrew and Zhang, Haoyu and Freedman, Michael None},
  journal={Proceedings of Machine Learning and Systems},
  volume={4},
  year={2022}
}

@article{pathways,
  title={Pathways: Asynchronous distributed dataflow for ML},
  author={Barham, Paul and Chowdhery, Aakanksha and Dean, Jeff and Ghemawat, Sanjay and Hand, Steven and Hurt, Dan and Isard, Michael and Lim, Hyeontaek and Pang, Ruoming and Roy, Sudip and others},
  journal={arXiv preprint arXiv:2203.12533},
  year={2022}
}

@inproceedings{whale,
  title={Whale: Efficient Giant Model Training over Heterogeneous $\{$GPUs$\}$},
  author={Jia, Xianyan and Jiang, Le and Wang, Ang and Xiao, Wencong and Shi, Ziji and Zhang, Jie and Li, Xinyuan and Chen, Langshi and Li, Yong and Zheng, Zhen and others},
  booktitle={2022 USENIX Annual Technical Conference (USENIX ATC 22)},
  pages={673--688},
  year={2022}
}

@article{taccl,
  title={Synthesizing collective communication algorithms for heterogeneous networks with taccl},
  author={Shah, Aashaka and Chidambaram, Vijay and Cowan, Meghan and Maleki, Saeed and Musuvathi, Madan and Mytkowicz, Todd and Nelson, Jacob and Saarikivi, Olli and Singh, Rachee},
  journal={arXiv preprint arXiv:2111.04867},
  year={2021}
}

@article{metis,
  title={A fast and high quality multilevel scheme for partitioning irregular graphs},
  author={Karypis, George and Kumar, Vipin},
  journal={SIAM Journal on scientific Computing},
  volume={20},
  number={1},
  pages={359--392},
  year={1998},
  publisher={SIAM}
}

@inproceedings{hdp,
  title={A hierarchical model for device placement},
  author={Mirhoseini, Azalia and Goldie, Anna and Pham, Hieu and Steiner, Benoit and Le, Quoc V and Dean, Jeff},
  booktitle={International Conference on Learning Representations},
  year={2018}
}

@inproceedings{hetpipe,
  title={$\{$HetPipe$\}$: Enabling Large $\{$DNN$\}$ Training on (Whimpy) Heterogeneous $\{$GPU$\}$ Clusters through Integration of Pipelined Model Parallelism and Data Parallelism},
  author={Park, Jay H and Yun, Gyeongchan and Chang, M Yi and Nguyen, Nguyen T and Lee, Seungmin and Choi, Jaesik and Noh, Sam H and Choi, Young-ri},
  booktitle={2020 USENIX Annual Technical Conference (USENIX ATC 20)},
  pages={307--321},
  year={2020}
}

@article{blueconnect,
  title={Blueconnect: Decomposing all-reduce for deep learning on heterogeneous network hierarchy},
  author={Cho, Minsik and Finkler, Ulrich and Kung, David and Hunter, Hillery},
  journal={Proceedings of Machine Learning and Systems},
  volume={1},
  pages={241--251},
  year={2019}
}

@article{torchfx,
  title={torch. fx: Practical Program Capture and Transformation for Deep Learning in Python},
  author={Reed, James and DeVito, Zachary and He, Horace and Ussery, Ansley and Ansel, Jason},
  journal={Proceedings of Machine Learning and Systems},
  volume={4},
  year={2022}
}

@article{vgg,
  title={Very deep convolutional networks for large-scale image recognition},
  author={Simonyan, Karen and Zisserman, Andrew},
  journal={arXiv preprint arXiv:1409.1556},
  year={2014}
}

@inproceedings{resnet,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}

@inproceedings{unity,
  title={Unity: Accelerating $\{$DNN$\}$ Training Through Joint Optimization of Algebraic Transformations and Parallelization},
  author={Unger, Colin and Jia, Zhihao and Wu, Wei and Lin, Sina and Baines, Mandeep and Narvaez, Carlos Efrain Quintero and Ramakrishnaiah, Vinay and Prajapati, Nirmal and McCormick, Pat and Mohd-Yusof, Jamaludin and others},
  booktitle={16th USENIX Symposium on Operating Systems Design and Implementation (OSDI 22)},
  pages={267--284},
  year={2022}
}

@article{hoare,
  title={An axiomatic basis for computer programming},
  author={Hoare, Charles Antony Richard},
  journal={Communications of the ACM},
  volume={12},
  number={10},
  pages={576--580},
  year={1969},
  publisher={ACM New York, NY, USA}
}

@book{syntaxguidedsynthesis,
  title={Syntax-guided synthesis},
  author={Alur, Rajeev and Bodik, Rastislav and Juniwal, Garvit and Martin, Milo MK and Raghothaman, Mukund and Seshia, Sanjit A and Singh, Rishabh and Solar-Lezama, Armando and Torlak, Emina and Udupa, Abhishek},
  year={2013},
  publisher={IEEE}
}

@article{emergent,
  title={Emergent abilities of large language models},
  author={Wei, Jason and Tay, Yi and Bommasani, Rishi and Raffel, Colin and Zoph, Barret and Borgeaud, Sebastian and Yogatama, Dani and Bosma, Maarten and Zhou, Denny and Metzler, Donald and others},
  journal={arXiv preprint arXiv:2206.07682},
  year={2022}
}

@article{palme,
  title={PaLM-E: An Embodied Multimodal Language Model},
  author={Driess, Danny and Xia, Fei and Sajjadi, Mehdi SM and Lynch, Corey and Chowdhery, Aakanksha and Ichter, Brian and Wahid, Ayzaan and Tompson, Jonathan and Vuong, Quan and Yu, Tianhe and others},
  journal={arXiv preprint arXiv:2303.03378},
  year={2023}
}

@inproceedings{hidup,
  title={Accelerating large-scale distributed neural network training with SPMD parallelism},
  author={Zhang, Shiwei and Diao, Lansong and Wu, Chuan and Wang, Siyu and Lin, Wei},
  booktitle={Proceedings of the 13th Symposium on Cloud Computing},
  pages={403--418},
  year={2022}
}

@article{attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@inproceedings{nccl,
  title={Nccl 2.0},
  author={Jeaugey, Sylvain},
  booktitle={GPU Technology Conference (GTC)},
  volume={2},
  year={2017}
}

@inproceedings{unet,
  title={U-net: Convolutional networks for biomedical image segmentation},
  author={Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
  booktitle={Medical Image Computing and Computer-Assisted Intervention--MICCAI 2015: 18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18},
  pages={234--241},
  year={2015},
  organization={Springer}
}

@article{tag,
  title={Expediting Distributed DNN Training With Device Topology-Aware Graph Deployment},
  author={Zhang, Shiwei and Yi, Xiaodong and Diao, Lansong and Wu, Chuan and Wang, Siyu and Lin, Wei},
  journal={IEEE Transactions on Parallel and Distributed Systems},
  volume={34},
  number={4},
  pages={1281--1293},
  year={2023},
  publisher={IEEE}
}

@article{sfb,
  title={Distributed machine learning via sufficient factor broadcasting},
  author={Xie, Pengtao and Kim, Jin Kyu and Zhou, Yi and Ho, Qirong and Kumar, Abhimanu and Yu, Yaoliang and Xing, Eric},
  journal={arXiv preprint arXiv:1511.08486},
  year={2015}
}

@inproceedings{chilimbi,
  title={Project adam: Building an efficient and scalable deep learning training system},
  author={Chilimbi, Trishul and Suzue, Yutaka and Apacible, Johnson and Kalyanaraman, Karthik},
  booktitle={11th $\{$USENIX$\}$ Symposium on Operating Systems Design and Implementation ($\{$OSDI$\}$ 14)},
  pages={571--582},
  year={2014}
}

@inproceedings{poseidon,
  title={Poseidon: An efficient communication architecture for distributed deep learning on $\{$GPU$\}$ clusters},
  author={Zhang, Hao and Zheng, Zeyu and Xu, Shizhen and Dai, Wei and Ho, Qirong and Liang, Xiaodan and Hu, Zhiting and Wei, Jinliang and Xie, Pengtao and Xing, Eric P},
  booktitle={2017 $\{$USENIX$\}$ Annual Technical Conference ($\{$USENIX$\}$$\{$ATC$\}$ 17)},
  pages={181--193},
  year={2017}
}

@article{taps,
  title={TAPS: Topology-Aware Intra-Operator Parallelism Strategy Searching Algorithm for Deep Neural Networks},
  author={Liang, Peng and Zheng, Hao and Su, Teng and Qiao, Linbo and Li, Dongsheng},
  journal={arXiv preprint arXiv:2301.04285},
  year={2023}
}

@article{sygus17,
  title={Sygus-comp 2017: Results and analysis},
  author={Alur, Rajeev and Fisman, Dana and Singh, Rishabh and Solar-Lezama, Armando},
  journal={arXiv preprint arXiv:1711.11438},
  year={2017}
}

@article{ddp,
  title={Pytorch distributed: Experiences on accelerating data parallel training},
  author={Li, Shen and Zhao, Yanli and Varma, Rohan and Salpekar, Omkar and Noordhuis, Pieter and Li, Teng and Paszke, Adam and Smith, Jeff and Vaughan, Brian and Damania, Pritam and others},
  journal={arXiv preprint arXiv:2006.15704},
  year={2020}
}

@article{cbc,
  title={coin-or/Cbc: Version 2.9. 9},
  author={Forrest, John and Ralphs, Ted and Vigerske, Stefan and Hafer, Lou and Kristjansson, Bjarni and Fasano, JP and Straver, E and Lubin, M and Santos, HG and Lougee, R and others},
  journal={URL http://dx.doi.org/10.5281/zenodo},
  volume={1317566},
  year={2018}
}

@inproceedings{ps,
  title={Scaling distributed machine learning with the parameter server},
  author={Li, Mu and Andersen, David G and Park, Jun Woo and Smola, Alexander J and Ahmed, Amr and Josifovski, Vanja and Long, James and Shekita, Eugene J and Su, Bor-Yiing},
  booktitle={11th $\{$USENIX$\}$ Symposium on Operating Systems Design and Implementation ($\{$OSDI$\}$ 14)},
  pages={583--598},
  year={2014}
}

@article{fmreduce,
  title={Efficient Partial Reduce Across Clouds},
  author={Wang, Renyi and Luo, Shouxi and Li, Ke and Xing, Huanlai},
  journal={6th Asia-Pacific Workshop on Networking (APNet 2022)},
  year={2022}
}

@article{oneweirdtrick,
  title={One weird trick for parallelizing convolutional neural networks},
  author={Krizhevsky, Alex},
  journal={arXiv preprint arXiv:1404.5997},
  year={2014}
}

@inproceedings{deepspeedmoe,
  title={Deepspeed-moe: Advancing mixture-of-experts inference and training to power next-generation ai scale},
  author={Rajbhandari, Samyam and Li, Conglong and Yao, Zhewei and Zhang, Minjia and Aminabadi, Reza Yazdani and Awan, Ammar Ahmad and Rasley, Jeff and He, Yuxiong},
  booktitle={International Conference on Machine Learning},
  pages={18332--18346},
  year={2022},
  organization={PMLR}
}

@article{megatron3,
  title={Reducing activation recomputation in large transformer models},
  author={Korthikanti, Vijay Anand and Casper, Jared and Lym, Sangkug and McAfee, Lawrence and Andersch, Michael and Shoeybi, Mohammad and Catanzaro, Bryan},
  journal={Proceedings of Machine Learning and Systems},
  volume={5},
  year={2023}
}

@article{attentionquadratic,
  title={Self-attention Does Not Need $O(n^2)$ Memory},
  author={Rabe, Markus N and Staats, Charles},
  journal={arXiv preprint arXiv:2112.05682},
  year={2021}
}

@article{longformer,
  title={Longformer: The long-document transformer},
  author={Beltagy, Iz and Peters, Matthew E and Cohan, Arman},
  journal={arXiv preprint arXiv:2004.05150},
  year={2020}
}

@inproceedings{quantization1,
  title={1-bit stochastic gradient descent and its application to data-parallel distributed training of speech dnns},
  author={Seide, Frank and Fu, Hao and Droppo, Jasha and Li, Gang and Yu, Dong},
  booktitle={Fifteenth Annual Conference of the International Speech Communication Association},
  year={2014}
}

@inproceedings{sparsification1,
  title={Scalable distributed DNN training using commodity GPU cloud computing},
  author={Strom, Nikko},
  booktitle={Sixteenth Annual Conference of the International Speech Communication Association},
  year={2015}
}

@article{blink,
  title={Blink: Fast and generic collectives for distributed ml},
  author={Wang, Guanhua and Venkataraman, Shivaram and Phanishayee, Amar and Devanur, Nikhil and Thelin, Jorgen and Stoica, Ion},
  journal={Proceedings of Machine Learning and Systems},
  volume={2},
  pages={172--186},
  year={2020}
}

@inproceedings{sccl,
  title={Synthesizing optimal collective algorithms},
  author={Cai, Zixian and Liu, Zhengyang and Maleki, Saeed and Musuvathi, Madanlal and Mytkowicz, Todd and Nelson, Jacob and Saarikivi, Olli},
  booktitle={Proceedings of the 26th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
  pages={62--75},
  year={2021}
}

@article{plink,
  title={Plink: Discovering and exploiting locality for accelerated distributed training on the public cloud},
  author={Luo, Liang and West, Peter and Nelson, Jacob and Krishnamurthy, Arvind and Ceze, Luis},
  journal={Proceedings of Machine Learning and Systems},
  volume={2},
  pages={82--97},
  year={2020}
}

@article{flashattention,
  title={Flashattention: Fast and memory-efficient exact attention with io-awareness},
  author={Dao, Tri and Fu, Dan and Ermon, Stefano and Rudra, Atri and R{\'e}, Christopher},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={16344--16359},
  year={2022}
}

@article{flashattention2,
  title={FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning},
  author={Dao, Tri},
  journal={arXiv preprint arXiv:2307.08691},
  year={2023}
}

@article{sparsetransformer,
  title={Generating long sequences with sparse transformers},
  author={Child, Rewon and Gray, Scott and Radford, Alec and Sutskever, Ilya},
  journal={arXiv preprint arXiv:1904.10509},
  year={2019}
}

@inproceedings{tvm,
  title={$\{$TVM$\}$: An automated $\{$End-to-End$\}$ optimizing compiler for deep learning},
  author={Chen, Tianqi and Moreau, Thierry and Jiang, Ziheng and Zheng, Lianmin and Yan, Eddie and Shen, Haichen and Cowan, Meghan and Wang, Leyuan and Hu, Yuwei and Ceze, Luis and others},
  booktitle={13th USENIX Symposium on Operating Systems Design and Implementation (OSDI 18)},
  pages={578--594},
  year={2018}
}

@inproceedings{taso,
  title={TASO: optimizing deep learning computation with automatic generation of graph substitutions},
  author={Jia, Zhihao and Padon, Oded and Thomas, James and Warszawski, Todd and Zaharia, Matei and Aiken, Alex},
  booktitle={Proceedings of the 27th ACM Symposium on Operating Systems Principles},
  pages={47--62},
  year={2019}
}

@inproceedings{triton,
  title={Triton: an intermediate language and compiler for tiled neural network computations},
  author={Tillet, Philippe and Kung, Hsiang-Tsung and Cox, David},
  booktitle={Proceedings of the 3rd ACM SIGPLAN International Workshop on Machine Learning and Programming Languages},
  pages={10--19},
  year={2019}
}

@article{halide,
  title={Halide: a language and compiler for optimizing parallelism, locality, and recomputation in image processing pipelines},
  author={Ragan-Kelley, Jonathan and Barnes, Connelly and Adams, Andrew and Paris, Sylvain and Durand, Fr{\'e}do and Amarasinghe, Saman},
  journal={Acm Sigplan Notices},
  volume={48},
  number={6},
  pages={519--530},
  year={2013},
  publisher={ACM New York, NY, USA}
}

@article{ios,
  title={Ios: Inter-operator scheduler for cnn acceleration},
  author={Ding, Yaoyao and Zhu, Ligeng and Jia, Zhihao and Pekhimenko, Gennady and Han, Song},
  journal={Proceedings of Machine Learning and Systems},
  volume={3},
  pages={167--180},
  year={2021}
}

@inproceedings{fastermoe,
  title={FasterMoE: modeling and optimizing training of large-scale dynamic pre-trained models},
  author={He, Jiaao and Zhai, Jidong and Antunes, Tiago and Wang, Haojie and Luo, Fuwen and Shi, Shangfeng and Li, Qin},
  booktitle={Proceedings of the 27th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
  pages={120--134},
  year={2022}
}

@article{disco,
  title={Optimizing DNN Compilation for Distributed Training With Joint OP and Tensor Fusion},
  author={Yi, Xiaodong and Zhang, Shiwei and Diao, Lansong and Wu, Chuan and Zheng, Zhen and Fan, Shiqing and Wang, Siyu and Yang, Jun and Lin, Wei},
  journal={IEEE Transactions on Parallel and Distributed Systems},
  volume={33},
  number={12},
  pages={4694--4706},
  year={2022},
  publisher={IEEE}
}

@inproceedings{overlapdecomposition,
  title={Overlap communication with dependent computation via decomposition in large deep learning models},
  author={Wang, Shibo and Wei, Jinliang and Sabne, Amit and Davis, Andy and Ilbeyi, Berkin and Hechtman, Blake and Chen, Dehao and Murthy, Karthik Srinivasa and Maggioni, Marcello and Zhang, Qiao and others},
  booktitle={Proceedings of the 28th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 1},
  pages={93--106},
  year={2022}
}

@article{oases,
  title={Automated Tensor Model Parallelism with Overlapped Communication for Efficient Foundation Model Training},
  author={Li, Shengwei and Lai, Zhiquan and Hao, Yanqi and Liu, Weijie and Ge, Keshi and Deng, Xiaoge and Li, Dongsheng and Lu, Kai},
  journal={arXiv preprint arXiv:2305.16121},
  year={2023}
}

@article{merak,
  title={Merak: An efficient distributed dnn training framework with automated 3d parallelism for giant foundation models},
  author={Lai, Zhiquan and Li, Shengwei and Tang, Xudong and Ge, Keshi and Liu, Weijie and Duan, Yabo and Qiao, Linbo and Li, Dongsheng},
  journal={IEEE Transactions on Parallel and Distributed Systems},
  volume={34},
  number={5},
  pages={1466--1478},
  year={2023},
  publisher={IEEE}
}

@inproceedings{spp,
  title={Efficient pipeline planning for expedited distributed dnn training},
  author={Luo, Ziyue and Yi, Xiaodong and Long, Guoping and Fan, Shiqing and Wu, Chuan and Yang, Jun and Lin, Wei},
  booktitle={IEEE INFOCOM 2022-IEEE Conference on Computer Communications},
  pages={340--349},
  year={2022},
  organization={IEEE}
}

@inproceedings{dapple,
  title={DAPPLE: A pipelined data parallel approach for training large models},
  author={Fan, Shiqing and Rong, Yi and Meng, Chen and Cao, Zongyan and Wang, Siyu and Zheng, Zhen and Wu, Chuan and Long, Guoping and Yang, Jun and Xia, Lixue and others},
  booktitle={Proceedings of the 26th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
  pages={431--445},
  year={2021}
}

@article{powersgd,
  title={PowerSGD: Practical low-rank gradient compression for distributed optimization},
  author={Vogels, Thijs and Karimireddy, Sai Praneeth and Jaggi, Martin},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@article{gradientcompressionsurvey,
  title={On the utility of gradient compression in distributed training systems},
  author={Agarwal, Saurabh and Wang, Hongyi and Venkataraman, Shivaram and Papailiopoulos, Dimitris},
  journal={Proceedings of Machine Learning and Systems},
  volume={4},
  pages={652--672},
  year={2022}
}
